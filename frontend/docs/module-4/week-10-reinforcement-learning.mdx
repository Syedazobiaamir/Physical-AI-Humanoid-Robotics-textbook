---
sidebar_position: 1
title: "Week 10: Reinforcement Learning for Robotics"
description: "Apply reinforcement learning techniques to robot control and manipulation"
---

import ChapterQuiz from '@site/src/components/ChapterQuiz';
import ChatSelection from '@site/src/components/ChatSelection';

# Week 10: Reinforcement Learning for Robotics

<ChatSelection chapterId="week-10-reinforcement-learning">

## Learning Objectives

By the end of this chapter, you will be able to:

1. Understand the fundamentals of reinforcement learning
2. Implement policy gradient methods for robot control
3. Apply Q-learning and DQN to robotics problems
4. Use simulation environments for training
5. Transfer learned policies to real robots

## Introduction

Reinforcement Learning (RL) enables robots to learn behaviors through trial and error, interacting with their environment to maximize cumulative rewards.

### RL Framework

```
┌─────────────────────────────────────────┐
│                                         │
│   Agent ─────► Action ─────► Environment
│     ▲                            │       │
│     │                            ▼       │
│     └─── Reward + Next State ◄───┘       │
│                                         │
└─────────────────────────────────────────┘
```

## Theory: RL Fundamentals

### Markov Decision Process (MDP)

An MDP is defined by the tuple (S, A, P, R, γ):

- **S**: State space
- **A**: Action space
- **P**: Transition probability P(s'|s,a)
- **R**: Reward function R(s,a,s')
- **γ**: Discount factor (0 < γ < 1)

### Value Functions

**State-Value Function:**
```
V(s) = E[∑ γ^t * R_t | s_0 = s]
```

**Action-Value Function:**
```
Q(s, a) = E[∑ γ^t * R_t | s_0 = s, a_0 = a]
```

### Policy Gradient Theorem

```
∇J(θ) = E[∇log π(a|s;θ) * Q(s,a)]
```

## Lab Tasks

### Task 1: Implement Q-Learning

Create a Q-learning agent for robot navigation:

```python
# q_learning_agent.py
import numpy as np
from typing import Tuple

class QLearningAgent:
    def __init__(self,
                 n_states: int,
                 n_actions: int,
                 learning_rate: float = 0.1,
                 discount_factor: float = 0.99,
                 epsilon: float = 0.1):
        """
        Initialize Q-learning agent

        Args:
            n_states: Number of discrete states
            n_actions: Number of actions
            learning_rate: Learning rate alpha
            discount_factor: Discount factor gamma
            epsilon: Exploration rate
        """
        self.Q = np.zeros((n_states, n_actions))
        self.alpha = learning_rate
        self.gamma = discount_factor
        self.epsilon = epsilon
        self.n_actions = n_actions

    def select_action(self, state: int) -> int:
        """Select action using epsilon-greedy policy"""
        if np.random.random() < self.epsilon:
            return np.random.randint(self.n_actions)
        return np.argmax(self.Q[state])

    def update(self, state: int, action: int,
               reward: float, next_state: int, done: bool):
        """Update Q-value"""
        if done:
            target = reward
        else:
            target = reward + self.gamma * np.max(self.Q[next_state])

        self.Q[state, action] += self.alpha * (target - self.Q[state, action])

    def train(self, env, n_episodes: int = 1000) -> list:
        """Train agent on environment"""
        rewards = []

        for episode in range(n_episodes):
            state = env.reset()
            total_reward = 0
            done = False

            while not done:
                action = self.select_action(state)
                next_state, reward, done, _ = env.step(action)
                self.update(state, action, reward, next_state, done)
                state = next_state
                total_reward += reward

            rewards.append(total_reward)

            # Decay epsilon
            self.epsilon = max(0.01, self.epsilon * 0.995)

        return rewards
```

### Task 2: Policy Gradient (REINFORCE)

Implement the REINFORCE algorithm:

```python
# reinforce_agent.py
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical

class PolicyNetwork(nn.Module):
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 64):
        super().__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return torch.softmax(self.fc3(x), dim=-1)

class REINFORCEAgent:
    def __init__(self,
                 state_dim: int,
                 action_dim: int,
                 learning_rate: float = 1e-3,
                 gamma: float = 0.99):
        self.policy = PolicyNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)
        self.gamma = gamma

        self.log_probs = []
        self.rewards = []

    def select_action(self, state: np.ndarray) -> int:
        """Select action from policy"""
        state = torch.FloatTensor(state).unsqueeze(0)
        probs = self.policy(state)
        dist = Categorical(probs)
        action = dist.sample()

        self.log_probs.append(dist.log_prob(action))
        return action.item()

    def update(self):
        """Update policy using collected trajectory"""
        # Compute returns
        returns = []
        G = 0
        for r in reversed(self.rewards):
            G = r + self.gamma * G
            returns.insert(0, G)

        returns = torch.tensor(returns)
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)

        # Policy gradient
        policy_loss = []
        for log_prob, G in zip(self.log_probs, returns):
            policy_loss.append(-log_prob * G)

        self.optimizer.zero_grad()
        loss = torch.stack(policy_loss).sum()
        loss.backward()
        self.optimizer.step()

        # Clear trajectory
        self.log_probs = []
        self.rewards = []

        return loss.item()

    def train(self, env, n_episodes: int = 1000):
        """Train agent"""
        episode_rewards = []

        for episode in range(n_episodes):
            state = env.reset()
            done = False
            total_reward = 0

            while not done:
                action = self.select_action(state)
                next_state, reward, done, _ = env.step(action)
                self.rewards.append(reward)
                state = next_state
                total_reward += reward

            loss = self.update()
            episode_rewards.append(total_reward)

            if (episode + 1) % 100 == 0:
                avg_reward = np.mean(episode_rewards[-100:])
                print(f"Episode {episode+1}, Avg Reward: {avg_reward:.2f}")

        return episode_rewards
```

### Task 3: Simulation with PyBullet

Train a robot arm in simulation:

```python
# pybullet_training.py
import pybullet as p
import pybullet_data
import numpy as np
import gym
from gym import spaces

class RobotArmEnv(gym.Env):
    def __init__(self, render: bool = False):
        super().__init__()

        # Connect to physics server
        if render:
            self.physics_client = p.connect(p.GUI)
        else:
            self.physics_client = p.connect(p.DIRECT)

        p.setAdditionalSearchPath(pybullet_data.getDataPath())

        # Define action and observation spaces
        self.action_space = spaces.Box(
            low=-1.0, high=1.0, shape=(3,), dtype=np.float32
        )
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(9,), dtype=np.float32
        )

        self.max_steps = 200
        self.step_count = 0

    def reset(self):
        p.resetSimulation()
        p.setGravity(0, 0, -9.81)

        # Load ground plane
        self.plane_id = p.loadURDF("plane.urdf")

        # Load robot (simplified arm)
        self.robot_id = self._load_robot()

        # Random target position
        self.target_pos = np.random.uniform(
            low=[0.3, -0.3, 0.2],
            high=[0.6, 0.3, 0.5]
        )
        self._create_target_visual()

        self.step_count = 0
        return self._get_observation()

    def _load_robot(self):
        """Load robot URDF"""
        # Simplified - in practice, load your robot URDF
        robot_id = p.loadURDF(
            "kuka_iiwa/model.urdf",
            basePosition=[0, 0, 0],
            useFixedBase=True
        )
        return robot_id

    def _create_target_visual(self):
        """Create visual marker for target"""
        visual_shape = p.createVisualShape(
            p.GEOM_SPHERE,
            radius=0.05,
            rgbaColor=[1, 0, 0, 0.7]
        )
        p.createMultiBody(
            baseMass=0,
            baseVisualShapeIndex=visual_shape,
            basePosition=self.target_pos
        )

    def _get_observation(self):
        """Get current observation"""
        # Get end-effector position
        ee_state = p.getLinkState(self.robot_id, 6)
        ee_pos = np.array(ee_state[0])

        # Get joint positions
        joint_states = p.getJointStates(self.robot_id, range(7))
        joint_pos = np.array([s[0] for s in joint_states[:3]])

        return np.concatenate([ee_pos, joint_pos, self.target_pos])

    def step(self, action):
        # Apply action as joint velocity commands
        for i, a in enumerate(action):
            p.setJointMotorControl2(
                self.robot_id,
                i,
                p.VELOCITY_CONTROL,
                targetVelocity=a * 2.0
            )

        p.stepSimulation()
        self.step_count += 1

        obs = self._get_observation()
        ee_pos = obs[:3]

        # Compute reward
        distance = np.linalg.norm(ee_pos - self.target_pos)
        reward = -distance

        # Success bonus
        if distance < 0.05:
            reward += 10.0

        done = self.step_count >= self.max_steps or distance < 0.05

        return obs, reward, done, {}

    def close(self):
        p.disconnect()
```

## Code Examples

### Deep Q-Network (DQN)

```python
# dqn_agent.py
import torch
import torch.nn as nn
import numpy as np
from collections import deque
import random

class DQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )

    def forward(self, x):
        return self.net(x)

class ReplayBuffer:
    def __init__(self, capacity: int = 10000):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size: int):
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        return (np.array(states), np.array(actions),
                np.array(rewards), np.array(next_states), np.array(dones))

    def __len__(self):
        return len(self.buffer)

class DQNAgent:
    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99):
        self.policy_net = DQN(state_dim, action_dim)
        self.target_net = DQN(state_dim, action_dim)
        self.target_net.load_state_dict(self.policy_net.state_dict())

        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=lr)
        self.buffer = ReplayBuffer()
        self.gamma = gamma
        self.action_dim = action_dim

    def select_action(self, state, epsilon=0.1):
        if random.random() < epsilon:
            return random.randrange(self.action_dim)
        with torch.no_grad():
            q_values = self.policy_net(torch.FloatTensor(state))
            return q_values.argmax().item()

    def update(self, batch_size=64):
        if len(self.buffer) < batch_size:
            return

        states, actions, rewards, next_states, dones = self.buffer.sample(batch_size)

        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions)
        rewards = torch.FloatTensor(rewards)
        next_states = torch.FloatTensor(next_states)
        dones = torch.FloatTensor(dones)

        # Current Q values
        current_q = self.policy_net(states).gather(1, actions.unsqueeze(1))

        # Target Q values
        with torch.no_grad():
            next_q = self.target_net(next_states).max(1)[0]
            target_q = rewards + (1 - dones) * self.gamma * next_q

        loss = nn.MSELoss()(current_q.squeeze(), target_q)

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

    def update_target(self):
        self.target_net.load_state_dict(self.policy_net.state_dict())
```

## Summary

In this chapter, we covered:

- Reinforcement learning fundamentals and MDP framework
- Q-learning and Deep Q-Networks (DQN)
- Policy gradient methods (REINFORCE)
- Simulation environments for robot training
- Practical considerations for sim-to-real transfer

## Additional Resources

- [Spinning Up in Deep RL](https://spinningup.openai.com/)
- [PyBullet Quickstart](https://pybullet.org/)
- [Stable Baselines3](https://stable-baselines3.readthedocs.io/)

</ChatSelection>

## Chapter Quiz

<ChapterQuiz chapterId="week-10-reinforcement-learning" />
