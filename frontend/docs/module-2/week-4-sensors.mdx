---
sidebar_position: 1
title: "Week 4: Sensor Integration"
description: "Learn to integrate and process data from various robot sensors in ROS2"
---

import ChapterQuiz from '@site/src/components/ChapterQuiz';
import ChatSelection from '@site/src/components/ChatSelection';

# Week 4: Sensor Integration

<ChatSelection chapterId="week-4-sensors">

## Learning Objectives

By the end of this chapter, you will be able to:

1. Understand common robot sensor types and their characteristics
2. Interface with IMUs, LiDAR, and depth cameras in ROS2
3. Process and filter sensor data for robot applications
4. Implement sensor fusion techniques
5. Calibrate sensors and handle sensor noise

## Introduction

Sensors are the eyes and ears of a robot. They provide the critical data needed for perception, navigation, and interaction with the environment. This chapter covers the integration of common robot sensors in ROS2.

## Theory: Robot Sensors

### Common Sensor Types

| Sensor | Data Type | Use Case | ROS2 Message |
|--------|-----------|----------|--------------|
| IMU | Orientation, acceleration | Motion tracking | `sensor_msgs/Imu` |
| LiDAR | Distance measurements | Mapping, obstacle detection | `sensor_msgs/LaserScan` |
| Depth Camera | RGB + Depth | 3D perception | `sensor_msgs/Image`, `sensor_msgs/PointCloud2` |
| Encoder | Position/velocity | Odometry | `nav_msgs/Odometry` |
| GPS | Global position | Outdoor navigation | `sensor_msgs/NavSatFix` |

### Sensor Characteristics

- **Accuracy**: How close measurements are to true values
- **Precision**: Repeatability of measurements
- **Frequency**: Update rate (Hz)
- **Range**: Measurement limits
- **Noise**: Random errors in measurements

## Lab Tasks

### Task 1: IMU Data Processing

Create a node that reads and filters IMU data:

```python
# imu_processor.py
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Imu
from geometry_msgs.msg import Vector3Stamped
import numpy as np
from collections import deque

class IMUProcessor(Node):
    def __init__(self):
        super().__init__('imu_processor')

        # Subscriber for raw IMU data
        self.subscription = self.create_subscription(
            Imu,
            'imu/data_raw',
            self.imu_callback,
            10)

        # Publisher for filtered orientation
        self.orientation_pub = self.create_publisher(
            Vector3Stamped,
            'imu/euler_angles',
            10)

        # Moving average filter buffers
        self.accel_buffer = deque(maxlen=10)
        self.gyro_buffer = deque(maxlen=10)

        # Complementary filter parameters
        self.alpha = 0.98  # Weight for gyroscope
        self.pitch = 0.0
        self.roll = 0.0
        self.last_time = None

        self.get_logger().info('IMU Processor started')

    def imu_callback(self, msg):
        # Extract data
        accel = np.array([
            msg.linear_acceleration.x,
            msg.linear_acceleration.y,
            msg.linear_acceleration.z
        ])
        gyro = np.array([
            msg.angular_velocity.x,
            msg.angular_velocity.y,
            msg.angular_velocity.z
        ])

        # Apply moving average filter
        self.accel_buffer.append(accel)
        self.gyro_buffer.append(gyro)

        filtered_accel = np.mean(self.accel_buffer, axis=0)
        filtered_gyro = np.mean(self.gyro_buffer, axis=0)

        # Calculate time delta
        current_time = self.get_clock().now()
        if self.last_time is not None:
            dt = (current_time - self.last_time).nanoseconds / 1e9

            # Complementary filter for pitch and roll
            accel_pitch = np.arctan2(filtered_accel[0],
                np.sqrt(filtered_accel[1]**2 + filtered_accel[2]**2))
            accel_roll = np.arctan2(filtered_accel[1], filtered_accel[2])

            self.pitch = self.alpha * (self.pitch + filtered_gyro[1] * dt) + \
                        (1 - self.alpha) * accel_pitch
            self.roll = self.alpha * (self.roll + filtered_gyro[0] * dt) + \
                       (1 - self.alpha) * accel_roll

            # Publish euler angles
            euler_msg = Vector3Stamped()
            euler_msg.header = msg.header
            euler_msg.vector.x = np.degrees(self.roll)
            euler_msg.vector.y = np.degrees(self.pitch)
            euler_msg.vector.z = 0.0  # Yaw requires magnetometer
            self.orientation_pub.publish(euler_msg)

        self.last_time = current_time

def main(args=None):
    rclpy.init(args=args)
    node = IMUProcessor()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

### Task 2: LiDAR Data Processing

Process LiDAR scans for obstacle detection:

```python
# lidar_processor.py
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import LaserScan
from std_msgs.msg import Float32MultiArray
from rclpy.qos import QoSProfile, ReliabilityPolicy
import numpy as np

class LidarProcessor(Node):
    def __init__(self):
        super().__init__('lidar_processor')

        # Use sensor data QoS for LiDAR
        sensor_qos = QoSProfile(depth=10)
        sensor_qos.reliability = ReliabilityPolicy.BEST_EFFORT

        self.subscription = self.create_subscription(
            LaserScan,
            'scan',
            self.scan_callback,
            sensor_qos)

        # Publisher for processed data
        self.sectors_pub = self.create_publisher(
            Float32MultiArray,
            'lidar/sectors',
            10)

        # Define sectors (8 sectors of 45 degrees each)
        self.num_sectors = 8

        self.get_logger().info('LiDAR Processor started')

    def scan_callback(self, msg):
        ranges = np.array(msg.ranges)

        # Replace inf and nan with max range
        ranges = np.where(np.isfinite(ranges), ranges, msg.range_max)

        # Divide into sectors
        sector_size = len(ranges) // self.num_sectors
        sector_mins = []

        for i in range(self.num_sectors):
            start = i * sector_size
            end = start + sector_size
            sector_data = ranges[start:end]

            # Filter out invalid readings
            valid_readings = sector_data[sector_data > msg.range_min]

            if len(valid_readings) > 0:
                sector_mins.append(float(np.min(valid_readings)))
            else:
                sector_mins.append(msg.range_max)

        # Publish sector data
        sector_msg = Float32MultiArray()
        sector_msg.data = sector_mins
        self.sectors_pub.publish(sector_msg)

        # Log closest obstacle
        min_distance = min(sector_mins)
        min_sector = sector_mins.index(min_distance)
        sector_angle = min_sector * (360 / self.num_sectors)

        if min_distance < 1.0:
            self.get_logger().warn(
                f'Obstacle at {min_distance:.2f}m in sector {min_sector} ({sector_angle}Â°)')

def main(args=None):
    rclpy.init(args=args)
    node = LidarProcessor()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

### Task 3: Depth Camera Integration

Process depth camera data for 3D perception:

```python
# depth_processor.py
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, PointCloud2, PointField
from cv_bridge import CvBridge
import numpy as np
import struct

class DepthProcessor(Node):
    def __init__(self):
        super().__init__('depth_processor')

        self.bridge = CvBridge()

        # Camera intrinsics (typical values, should be calibrated)
        self.fx = 525.0  # Focal length x
        self.fy = 525.0  # Focal length y
        self.cx = 320.0  # Principal point x
        self.cy = 240.0  # Principal point y

        self.subscription = self.create_subscription(
            Image,
            'camera/depth/image_raw',
            self.depth_callback,
            10)

        self.pointcloud_pub = self.create_publisher(
            PointCloud2,
            'camera/points',
            10)

        self.get_logger().info('Depth Processor started')

    def depth_callback(self, msg):
        # Convert depth image to numpy array
        depth_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='passthrough')

        # Create point cloud
        height, width = depth_image.shape

        # Generate pixel coordinates
        u = np.arange(width)
        v = np.arange(height)
        u, v = np.meshgrid(u, v)

        # Convert to 3D coordinates
        z = depth_image.astype(np.float32) / 1000.0  # mm to meters
        x = (u - self.cx) * z / self.fx
        y = (v - self.cy) * z / self.fy

        # Filter out invalid points
        valid = (z > 0.1) & (z < 10.0)

        points = np.stack([x[valid], y[valid], z[valid]], axis=-1)

        # Create PointCloud2 message
        cloud_msg = self.create_pointcloud2(msg.header, points)
        self.pointcloud_pub.publish(cloud_msg)

    def create_pointcloud2(self, header, points):
        msg = PointCloud2()
        msg.header = header
        msg.height = 1
        msg.width = len(points)

        msg.fields = [
            PointField(name='x', offset=0, datatype=PointField.FLOAT32, count=1),
            PointField(name='y', offset=4, datatype=PointField.FLOAT32, count=1),
            PointField(name='z', offset=8, datatype=PointField.FLOAT32, count=1),
        ]

        msg.is_bigendian = False
        msg.point_step = 12
        msg.row_step = msg.point_step * len(points)
        msg.is_dense = True

        msg.data = points.astype(np.float32).tobytes()

        return msg

def main(args=None):
    rclpy.init(args=args)
    node = DepthProcessor()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Code Examples

### Sensor Fusion: IMU + Odometry

```python
# sensor_fusion.py
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Imu
from nav_msgs.msg import Odometry
from geometry_msgs.msg import PoseWithCovarianceStamped
import numpy as np
from scipy.spatial.transform import Rotation

class SensorFusion(Node):
    def __init__(self):
        super().__init__('sensor_fusion')

        # Extended Kalman Filter state: [x, y, theta, v, omega]
        self.state = np.zeros(5)
        self.covariance = np.eye(5) * 0.1

        # Process noise
        self.Q = np.diag([0.01, 0.01, 0.001, 0.1, 0.01])

        # Measurement noise
        self.R_odom = np.diag([0.1, 0.1, 0.05])
        self.R_imu = np.diag([0.01])

        self.last_time = None

        # Subscribers
        self.odom_sub = self.create_subscription(
            Odometry, 'odom', self.odom_callback, 10)
        self.imu_sub = self.create_subscription(
            Imu, 'imu/data', self.imu_callback, 10)

        # Publisher
        self.pose_pub = self.create_publisher(
            PoseWithCovarianceStamped, 'fused_pose', 10)

        self.get_logger().info('Sensor Fusion started')

    def predict(self, dt):
        """EKF prediction step"""
        x, y, theta, v, omega = self.state

        # State prediction
        self.state[0] = x + v * np.cos(theta) * dt
        self.state[1] = y + v * np.sin(theta) * dt
        self.state[2] = theta + omega * dt

        # Jacobian of motion model
        F = np.eye(5)
        F[0, 2] = -v * np.sin(theta) * dt
        F[0, 3] = np.cos(theta) * dt
        F[1, 2] = v * np.cos(theta) * dt
        F[1, 3] = np.sin(theta) * dt
        F[2, 4] = dt

        # Covariance prediction
        self.covariance = F @ self.covariance @ F.T + self.Q

    def update_odom(self, z):
        """EKF update with odometry measurement"""
        # Measurement matrix for [x, y, theta]
        H = np.array([
            [1, 0, 0, 0, 0],
            [0, 1, 0, 0, 0],
            [0, 0, 1, 0, 0]
        ])

        # Innovation
        y = z - H @ self.state
        y[2] = np.arctan2(np.sin(y[2]), np.cos(y[2]))  # Normalize angle

        # Kalman gain
        S = H @ self.covariance @ H.T + self.R_odom
        K = self.covariance @ H.T @ np.linalg.inv(S)

        # State update
        self.state = self.state + K @ y
        self.covariance = (np.eye(5) - K @ H) @ self.covariance

    def odom_callback(self, msg):
        current_time = self.get_clock().now()

        if self.last_time is not None:
            dt = (current_time - self.last_time).nanoseconds / 1e9
            self.predict(dt)

        # Extract odometry measurement
        x = msg.pose.pose.position.x
        y = msg.pose.pose.position.y
        q = msg.pose.pose.orientation
        theta = Rotation.from_quat([q.x, q.y, q.z, q.w]).as_euler('xyz')[2]

        self.update_odom(np.array([x, y, theta]))
        self.state[3] = msg.twist.twist.linear.x  # Update velocity

        self.publish_pose(msg.header)
        self.last_time = current_time

    def imu_callback(self, msg):
        # Update angular velocity from IMU
        self.state[4] = msg.angular_velocity.z

    def publish_pose(self, header):
        pose_msg = PoseWithCovarianceStamped()
        pose_msg.header = header
        pose_msg.pose.pose.position.x = self.state[0]
        pose_msg.pose.pose.position.y = self.state[1]

        q = Rotation.from_euler('z', self.state[2]).as_quat()
        pose_msg.pose.pose.orientation.x = q[0]
        pose_msg.pose.pose.orientation.y = q[1]
        pose_msg.pose.pose.orientation.z = q[2]
        pose_msg.pose.pose.orientation.w = q[3]

        # Flatten covariance for message
        cov_3x3 = self.covariance[:3, :3]
        pose_msg.pose.covariance = [0.0] * 36
        for i in range(3):
            for j in range(3):
                pose_msg.pose.covariance[i*6 + j] = cov_3x3[i, j]

        self.pose_pub.publish(pose_msg)

def main(args=None):
    rclpy.init(args=args)
    node = SensorFusion()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Summary

In this chapter, we covered:

- Common robot sensor types and their characteristics
- Processing IMU data with complementary filtering
- LiDAR scan processing for obstacle detection
- Depth camera to point cloud conversion
- Sensor fusion using Extended Kalman Filter

## Additional Resources

- [ROS2 Sensor Messages](https://github.com/ros2/common_interfaces/tree/humble/sensor_msgs)
- [Robot Localization Package](https://github.com/cra-ros-pkg/robot_localization)
- [PCL (Point Cloud Library)](https://pointclouds.org/)

</ChatSelection>

## Chapter Quiz

<ChapterQuiz chapterId="week-4-sensors" />
